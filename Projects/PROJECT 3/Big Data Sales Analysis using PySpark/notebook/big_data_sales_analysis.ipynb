{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c14cc16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-CAJUM7L:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Big Data Sales Analysis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2873deef190>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Big Data Sales Analysis\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25fd6997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+----------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+--------+--------+--------+--------+\n",
      "|Row ID|      Order ID|Order Date| Ship Date|     Ship Mode|Customer ID|  Customer Name|  Segment|      Country|           City|     State|Postal Code|Region|     Product ID|       Category|Sub-Category|        Product Name|   Sales|Quantity|Discount|  Profit|\n",
      "+------+--------------+----------+----------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+--------+--------+--------+--------+\n",
      "|     1|CA-2013-152156|09-11-2013|12-11-2013|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|      42420| South|FUR-BO-10001798|      Furniture|   Bookcases|Bush Somerset Col...|  261.96|       2|       0| 41.9136|\n",
      "|     2|CA-2013-152156|09-11-2013|12-11-2013|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|      42420| South|FUR-CH-10000454|      Furniture|      Chairs|Hon Deluxe Fabric...|  731.94|       3|       0| 219.582|\n",
      "|     3|CA-2013-138688|13-06-2013|17-06-2013|  Second Class|   DV-13045|Darrin Van Huff|Corporate|United States|    Los Angeles|California|      90036|  West|OFF-LA-10000240|Office Supplies|      Labels|Self-Adhesive Add...|   14.62|       2|       0|  6.8714|\n",
      "|     4|US-2012-108966|11-10-2012|18-10-2012|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|      33311| South|FUR-TA-10000577|      Furniture|      Tables|Bretford CR4500 S...|957.5775|       5|    0.45|-383.031|\n",
      "|     5|US-2012-108966|11-10-2012|18-10-2012|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|      33311| South|OFF-ST-10000760|Office Supplies|     Storage|Eldon Fold 'N Rol...|  22.368|       2|     0.2|  2.5164|\n",
      "+------+--------------+----------+----------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+--------+--------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Superstore sales data\n",
    "df = spark.read.csv(\n",
    "    \"../data/superstore.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39ed3774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Row ID: integer (nullable = true)\n",
      " |-- Order ID: string (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Ship Date: string (nullable = true)\n",
      " |-- Ship Mode: string (nullable = true)\n",
      " |-- Customer ID: string (nullable = true)\n",
      " |-- Customer Name: string (nullable = true)\n",
      " |-- Segment: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Postal Code: integer (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Product ID: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Sub-Category: string (nullable = true)\n",
      " |-- Product Name: string (nullable = true)\n",
      " |-- Sales: string (nullable = true)\n",
      " |-- Quantity: string (nullable = true)\n",
      " |-- Discount: string (nullable = true)\n",
      " |-- Profit: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Row ID',\n",
       " 'Order ID',\n",
       " 'Order Date',\n",
       " 'Ship Date',\n",
       " 'Ship Mode',\n",
       " 'Customer ID',\n",
       " 'Customer Name',\n",
       " 'Segment',\n",
       " 'Country',\n",
       " 'City',\n",
       " 'State',\n",
       " 'Postal Code',\n",
       " 'Region',\n",
       " 'Product ID',\n",
       " 'Category',\n",
       " 'Sub-Category',\n",
       " 'Product Name',\n",
       " 'Sales',\n",
       " 'Quantity',\n",
       " 'Discount',\n",
       " 'Profit']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check schema (data types)\n",
    "df.printSchema()\n",
    "\n",
    "# Check column names\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57834ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of rows\n",
    "df.count()\n",
    "\n",
    "# Number of columns\n",
    "len(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9547993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+---------+---------+-----------+-------------+-------+-------+----+-----+-----------+------+----------+--------+------------+------------+-----+--------+--------+------+\n",
      "|Row ID|Order ID|Order Date|Ship Date|Ship Mode|Customer ID|Customer Name|Segment|Country|City|State|Postal Code|Region|Product ID|Category|Sub-Category|Product Name|Sales|Quantity|Discount|Profit|\n",
      "+------+--------+----------+---------+---------+-----------+-------------+-------+-------+----+-----+-----------+------+----------+--------+------------+------------+-----+--------+--------+------+\n",
      "|     0|       0|         0|        0|        0|          0|            0|      0|      0|   0|    0|          0|     0|         0|       0|           0|           0|    0|       0|       0|     0|\n",
      "+------+--------+----------+---------+---------+-----------+-------------+-------+-------+----+-----+-----------+------+----------+--------+------------+------------+-----+--------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Count nulls per column\n",
    "null_counts = df.select([\n",
    "    sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns\n",
    "])\n",
    "\n",
    "null_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f2b95c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---------------+------------+--------+--------+\n",
      "|Order Date|Region|       Category|Sub-Category|   Sales|  Profit|\n",
      "+----------+------+---------------+------------+--------+--------+\n",
      "|09-11-2013| South|      Furniture|   Bookcases|  261.96| 41.9136|\n",
      "|09-11-2013| South|      Furniture|      Chairs|  731.94| 219.582|\n",
      "|13-06-2013|  West|Office Supplies|      Labels|   14.62|  6.8714|\n",
      "|11-10-2012| South|      Furniture|      Tables|957.5775|-383.031|\n",
      "|11-10-2012| South|Office Supplies|     Storage|  22.368|  2.5164|\n",
      "+----------+------+---------------+------------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select key business columns\n",
    "selected_cols = [\n",
    "    \"Order Date\",\n",
    "    \"Region\",\n",
    "    \"Category\",\n",
    "    \"Sub-Category\",\n",
    "    \"Sales\",\n",
    "    \"Profit\"\n",
    "]\n",
    "\n",
    "df_selected = df.select(selected_cols)\n",
    "df_selected.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "086b51eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Order Date: date (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Sub-Category: string (nullable = true)\n",
      " |-- Sales: string (nullable = true)\n",
      " |-- Profit: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# Convert Order Date from string to date\n",
    "df_selected = df_selected.withColumn(\n",
    "    \"Order Date\",\n",
    "    to_date(\"Order Date\", \"MM/dd/yyyy\")\n",
    ")\n",
    "\n",
    "df_selected.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06e66e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|             Sales|            Profit|\n",
      "+-------+------------------+------------------+\n",
      "|  count|              9994|              9994|\n",
      "|   mean|234.41818199917006|28.587912967780834|\n",
      "| stddev| 631.7890112674363| 234.3891156047269|\n",
      "|    min|          10/Pack\"|         -6599.978|\n",
      "|    max|            999.98|          8399.976|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check basic statistics for Sales and Profit\n",
    "df_selected.select(\"Sales\", \"Profit\").describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2b96584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9994"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove rows where Sales or Profit is null (safety step)\n",
    "df_clean = df_selected.dropna(subset=[\"Sales\", \"Profit\"])\n",
    "\n",
    "# Quick check\n",
    "df_clean.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d39d68a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|       Total_Sales|      Total_Profit|\n",
      "+------------------+------------------+\n",
      "|2272449.8562999545|285707.60220000165|\n",
      "+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Overall sales and profit\n",
    "df_clean.select(\n",
    "    sum(\"Sales\").alias(\"Total_Sales\"),\n",
    "    sum(\"Profit\").alias(\"Total_Profit\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83534df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "| Region|       Total_Sales|      Total_Profit|\n",
      "+-------+------------------+------------------+\n",
      "|  South|388983.58500000037|46650.341000000044|\n",
      "|Central| 497800.8728000007| 40150.50299999999|\n",
      "|   East| 672194.0539999981| 91603.05670000015|\n",
      "|   West| 713471.3445000004|107303.70150000004|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Region-wise sales and profit\n",
    "df_clean.groupBy(\"Region\").agg(\n",
    "    sum(\"Sales\").alias(\"Total_Sales\"),\n",
    "    sum(\"Profit\").alias(\"Total_Profit\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1eaa1eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+------------------+\n",
      "|       Category|      Total_Sales|      Total_Profit|\n",
      "+---------------+-----------------+------------------+\n",
      "|Office Supplies|703502.9280000031|120632.87839999991|\n",
      "|      Furniture|733046.8612999996| 19686.42720000003|\n",
      "|     Technology|835900.0669999964|145388.29659999989|\n",
      "+---------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Category-wise sales and profit\n",
    "df_clean.groupBy(\"Category\").agg(\n",
    "    sum(\"Sales\").alias(\"Total_Sales\"),\n",
    "    sum(\"Profit\").alias(\"Total_Profit\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6ce1fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+-------------------+\n",
      "|Sub-Category|       Total_Sales|       Total_Profit|\n",
      "+------------+------------------+-------------------+\n",
      "|   Envelopes|15339.489999999993|  6461.269100000003|\n",
      "|         Art|27118.791999999954|  6527.786999999998|\n",
      "|      Chairs|328449.10300000076| 26590.166300000026|\n",
      "| Furnishings| 82752.23000000004| 14294.297999999995|\n",
      "|    Supplies| 45952.47000000001|-1347.3654999999983|\n",
      "|   Fasteners|3008.6559999999995|  942.6377999999997|\n",
      "|     Binders|199905.71700000006| 30038.821299999996|\n",
      "|   Bookcases|114879.99629999997|-3472.5559999999978|\n",
      "|      Labels|         12486.312|  5546.253999999998|\n",
      "|       Paper| 75356.11799999999|  32795.56099999999|\n",
      "| Accessories| 167380.3180000001|  41936.63569999993|\n",
      "|     Copiers|149528.02999999994|  55617.82490000001|\n",
      "|      Phones| 329753.0880000001|         44449.0791|\n",
      "|    Machines|189238.63099999996|          3384.7569|\n",
      "|     Storage|216803.21200000012|         21529.9083|\n",
      "|  Appliances|        107532.161| 18138.005399999995|\n",
      "|      Tables| 206965.5320000001|-17725.481100000008|\n",
      "+------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Sub-Category-wise sales and profit\n",
    "df_clean.groupBy(\"Sub-Category\").agg(\n",
    "    sum(\"Sales\").alias(\"Total_Sales\"),\n",
    "    sum(\"Profit\").alias(\"Total_Profit\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "486e7a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+------------------+------------------+\n",
      "| Region|       Category|       Total_Sales|      Total_Profit|\n",
      "+-------+---------------+------------------+------------------+\n",
      "|  South|Office Supplies|123979.92499999993| 19595.75349999999|\n",
      "|Central|Office Supplies|164616.19700000016| 9038.715399999997|\n",
      "|  South|      Furniture| 116273.1360000001| 7071.571899999995|\n",
      "|   West|Office Supplies|213125.18300000002| 51211.95060000014|\n",
      "|Central|     Technology| 170401.5319999999|33693.441399999996|\n",
      "|Central|      Furniture|162783.14380000005|        -2581.6538|\n",
      "|  South|     Technology|148730.52399999992|19983.015600000006|\n",
      "|   West|      Furniture|248450.23350000026|        11819.8689|\n",
      "|   West|     Technology|251895.92799999993| 44271.88199999997|\n",
      "|   East|Office Supplies|201781.62299999985| 40786.45889999996|\n",
      "|   East|      Furniture|205540.34800000011|3376.6402000000016|\n",
      "|   East|     Technology|264872.08300000033|47439.957599999936|\n",
      "+-------+---------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Region + Category-wise sales and profit\n",
    "df_clean.groupBy(\"Region\", \"Category\").agg(\n",
    "    sum(\"Sales\").alias(\"Total_Sales\"),\n",
    "    sum(\"Profit\").alias(\"Total_Profit\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33ee7742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+------------------+------------------+\n",
      "| Region|       Category|       Total_Sales|      Total_Profit|\n",
      "+-------+---------------+------------------+------------------+\n",
      "|   East|     Technology|264872.08300000033|47439.957599999936|\n",
      "|   West|     Technology|251895.92799999993| 44271.88199999997|\n",
      "|   West|      Furniture|248450.23350000026|        11819.8689|\n",
      "|   West|Office Supplies|213125.18300000002| 51211.95060000014|\n",
      "|   East|      Furniture|205540.34800000011|3376.6402000000016|\n",
      "|   East|Office Supplies|201781.62299999985| 40786.45889999996|\n",
      "|Central|     Technology| 170401.5319999999|33693.441399999996|\n",
      "|Central|Office Supplies|164616.19700000016| 9038.715399999997|\n",
      "|Central|      Furniture|162783.14380000005|        -2581.6538|\n",
      "|  South|     Technology|148730.52399999992|19983.015600000006|\n",
      "+-------+---------------+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Top Region-Category pairs by Sales\n",
    "df_clean.groupBy(\"Region\", \"Category\").agg(\n",
    "    sum(\"Sales\").alias(\"Total_Sales\"),\n",
    "    sum(\"Profit\").alias(\"Total_Profit\")\n",
    ").orderBy(\"Total_Sales\", ascending=False).show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4ed3bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "| Region|      Total_Profit|\n",
      "+-------+------------------+\n",
      "|   West|107303.70150000004|\n",
      "|   East| 91603.05670000015|\n",
      "|  South|46650.341000000044|\n",
      "|Central| 40150.50299999999|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Region-wise profit (focus on profitability)\n",
    "df_clean.groupBy(\"Region\").agg(\n",
    "    sum(\"Profit\").alias(\"Total_Profit\")\n",
    ").orderBy(\"Total_Profit\", ascending=False).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "522154ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|       Category|      Total_Profit|\n",
      "+---------------+------------------+\n",
      "|     Technology|145388.29659999989|\n",
      "|Office Supplies|120632.87839999991|\n",
      "|      Furniture| 19686.42720000003|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Category-wise profit\n",
    "df_clean.groupBy(\"Category\").agg(\n",
    "    sum(\"Profit\").alias(\"Total_Profit\")\n",
    ").orderBy(\"Total_Profit\", ascending=False).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a13d591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+------------------+\n",
      "| Region|       Category|      Total_Profit|\n",
      "+-------+---------------+------------------+\n",
      "|   West|Office Supplies| 51211.95060000014|\n",
      "|   East|     Technology|47439.957599999936|\n",
      "|   West|     Technology| 44271.88199999997|\n",
      "|   East|Office Supplies| 40786.45889999996|\n",
      "|Central|     Technology|33693.441399999996|\n",
      "|  South|     Technology|19983.015600000006|\n",
      "|  South|Office Supplies| 19595.75349999999|\n",
      "|   West|      Furniture|        11819.8689|\n",
      "|Central|Office Supplies| 9038.715399999997|\n",
      "|  South|      Furniture| 7071.571899999995|\n",
      "|   East|      Furniture|3376.6402000000016|\n",
      "|Central|      Furniture|        -2581.6538|\n",
      "+-------+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Region + Category profitability\n",
    "df_clean.groupBy(\"Region\", \"Category\").agg(\n",
    "    sum(\"Profit\").alias(\"Total_Profit\")\n",
    ").orderBy(\"Total_Profit\", ascending=False).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf028c37",
   "metadata": {},
   "source": [
    "## Business Insight\n",
    "Technology category drives the highest profit, especially in East and West regions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
